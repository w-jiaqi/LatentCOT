dataset: 4x4
latent_pool: 8
epochs: 3

model: meta-llama/Llama-3.2-1B
tokenizer: meta-llama/Llama-3.2-1B

num_train: null # use all training examples
batch_num: 32

checkpoints_dir: checkpoints
checkpoints_name: 4x4-pool_8-no_smoothing

lr: 1.0e-5

no_cache: True
position_smoothing: False