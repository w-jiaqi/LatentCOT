dataset: 4x4
latent_pool: 8
epochs: 3

model: meta-llama/Llama-3.2-1B
tokenizer: meta-llama/Llama-3.2-1B

num_train: null # use all training examples
batch_num: 32

checkpoints_dir: checkpoints
checkpoints_name: pool_8

lr: 1.0e-5

no_cache: True