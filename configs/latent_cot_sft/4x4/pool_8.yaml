dataset: 4x4
latent_pool: 8
epochs: 3

model: meta-llama/Llama-3.2-1B
tokenizer: meta-llama/Llama-3.2-1B

num_train: null # use all training examples
batch_num: 32

checkpoints_dir: checkpoints
checkpoints_name: pool_8

latents_lr: 1.0e-5
token_lr: 1.0e-5

no_cache: False

skip_latents: False
skip_tokens: False