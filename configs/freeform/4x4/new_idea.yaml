dataset: 4x4
model: meta-llama/Llama-3.2-1B
tokenizer: meta-llama/Llama-3.2-1B
epochs: 5
checkpoints_dir: checkpoints
batch_num: 32
max_new_latents: 8
checkpoints_name: new_idea
save_steps: 10000
lr: 1.0e-6
freeze_embeddings: true
unembed_latents: true
dynamically_stop: false
answer_loss_scaling: 2.0
