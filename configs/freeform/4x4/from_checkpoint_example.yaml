dataset: 4x4
epochs: 5
max_new_latents: 8
batch_num: 32

model: meta-llama/Llama-3.2-1B
model_pth: {{MODEL_PTH_CHECKPOINT}}
tokenizer: meta-llama/Llama-3.2-1B

checkpoints_dir: checkpoints
checkpoints_name: {{CHECKPOINTS_NAME}}

save_steps: 10000
lr: 1.0e-6

freeze_embeddings: true
unembed_latents: true
dynamically_stop: false