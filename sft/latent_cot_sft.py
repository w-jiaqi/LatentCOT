# -*- coding: utf-8 -*-
"""LatentCOT_SFT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k1IWGGdtPfMNyFZsNWIZkqLpoWglV-Tk
"""

print("Importing")

import torch
import datasets

from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F

from peft import LoraConfig, get_peft_model

from collections import namedtuple
from tqdm.auto import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules="all-linear",
    task_type="CAUSAL_LM",
)

model_name = "meta-llama/Llama-3.2-1B-Instruct"

print("Loading model")

base_model = AutoModelForCausalLM.from_pretrained(model_name)
model = get_peft_model(base_model, peft_config)
model.print_trainable_parameters()

print("Loading tokenizer")
tokenizer = AutoTokenizer.from_pretrained(model_name)

print("Loading dataset")
base_dataset = datasets.load_dataset("text", data_files={"train": "data/multiplication/4x4/train.txt", "test": "data/multiplication/4x4/valid.txt"})

# TODO: MAKE SURE TO CHANGE THIS BACK LATER
TRAIN_SIZE = 10000
base_dataset['train'] = base_dataset['train'].select(range(TRAIN_SIZE))

def process_base_dataset(example):
    text = example['text']

    question = text.split('||')[0].strip()
    full_answer = text.split('||')[1]

    reasoning = full_answer.split('####')[0].strip()
    answer = full_answer.split('####')[1].strip()

    return {
        "question": question,
        "reasoning": reasoning,
        "answer": answer
    }

print("Processing dataset")
base_dataset = base_dataset.map(process_base_dataset, remove_columns=base_dataset['train'].features)

tokenizer.add_special_tokens({"pad_token":"<|pad|>"})
tokenizer.add_tokens("<|begin_latent|>")
tokenizer.add_tokens("<|end_latent|>")
tokenizer.add_tokens("<|latent|>")

pad_id = tokenizer.convert_tokens_to_ids("<|pad|>")
begin_latent_id = tokenizer.convert_tokens_to_ids("<|begin_latent|>")
end_latent_id = tokenizer.convert_tokens_to_ids("<|end_latent|>")
latent_id = tokenizer.convert_tokens_to_ids("<|latent|>")

model.config.pad_token_id=pad_id

model.resize_token_embeddings(len(tokenizer))

LATENT_POOL = 10 # how many tokens we should pool together to form our starting latents

embedding = model.get_input_embeddings()

# TODO: optimize this
def process_cot_latent_dataset(example):
    question = example['question']
    reasoning = example['reasoning']

    question_token_ids = tokenizer.encode(question, return_tensors="pt")[0] # get rid of batch dimension
    reasoning_token_ids = tokenizer.encode(reasoning, return_tensors="pt")[0]

    reasoning_embeddings = embedding(reasoning_token_ids)

    latent_dim = reasoning_embeddings.shape[-1]

    reasoning_length = reasoning_embeddings.shape[0]
    latent_reasoning_length = (reasoning_length // LATENT_POOL) + 1 # pool remaining tokens instead of truncating

    latent_reasoning_embeddings = torch.zeros(latent_reasoning_length, latent_dim)

    for i in range(0, reasoning_length, LATENT_POOL):
        latent_reasoning_embeddings[i // LATENT_POOL, :] = reasoning_embeddings[i:i+LATENT_POOL, :].mean(dim=0)

    assert(latent_reasoning_embeddings.shape[0] == latent_reasoning_length)

    input_ids = torch.cat((question_token_ids, torch.tensor([begin_latent_id]), torch.tensor(latent_reasoning_length * [latent_id]), torch.tensor([end_latent_id])))
    latent_tensor = torch.cat((torch.zeros(len(question_token_ids) + 1, latent_dim), latent_reasoning_embeddings, torch.zeros(1, latent_dim)), dim=0)
    latent_mask = torch.cat((torch.ones(len(question_token_ids) + 1, latent_dim), torch.zeros(latent_reasoning_length, latent_dim), torch.ones(1, latent_dim)), dim=0)
    attention_mask = torch.ones(input_ids.shape)

    return {
        "input_ids": input_ids,
        "latent_tensor": latent_tensor,
        "latent_mask": latent_mask,
        "attention_mask": attention_mask
    }

print("Processing latent dataset")
cot_latent_dataset = base_dataset.map(process_cot_latent_dataset, remove_columns=base_dataset['train'].features, cache_file_names={'train':'ds_cache/4x4_multiplication_latent_train.arrow', 'test':'ds_cache/4x4_multiplication_latent_eval.arrow'})
cot_latent_dataset.set_format("pt")

pad_token_id = tokenizer.eos_token_id

def cot_latent_collate_fn(batch):
    longest_seq = max(len(example['input_ids']) for example in batch)

    for example in batch:
        example['input_ids'] = torch.cat((example['input_ids'], torch.full([longest_seq - len(example['input_ids'])], pad_token_id)))
        example['latent_tensor'] = torch.cat((example['latent_tensor'], torch.zeros(longest_seq - len(example['latent_tensor']), example['latent_tensor'].shape[-1])))
        example['latent_mask'] = torch.cat((example['latent_mask'], torch.zeros(longest_seq - len(example['latent_mask']), example['latent_mask'].shape[-1])))
        example['attention_mask'] = torch.cat((example['attention_mask'], torch.zeros(longest_seq - len(example['attention_mask']), dtype=torch.int8)))


    input_ids = torch.stack([example['input_ids'] for example in batch])
    latent_tensor = torch.stack([example['latent_tensor'] for example in batch])
    latent_mask = torch.stack([example['latent_mask'] for example in batch])
    attention_mask = torch.stack([example['attention_mask'] for example in batch])

    return {
        "input_ids": input_ids,
        "latent_tensor": latent_tensor,
        "latent_mask": latent_mask,
        "attention_mask": attention_mask
    }

cot_latent_dataloader = DataLoader(cot_latent_dataset['train'], batch_size=32, shuffle=False, collate_fn=cot_latent_collate_fn)

class LatentCOT(nn.Module):
    def __init__(self):
        super(LatentCOT, self).__init__()
    def forward(self, input_ids, attention_mask, latent_tensor, latent_mask):
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        latent_tensor = latent_tensor.to(device)
        latent_mask = latent_mask.to(device)

        if device == 'cpu':
            print("CURRENTLY USING CPU")

        src_input_ids = input_ids[:, :-1]
        tgt_input_ids = input_ids[:, 1:]

        src_input_embeddings = embedding(src_input_ids)
        tgt_input_embeddings = embedding(tgt_input_ids)

        src_input_embeddings = torch.mul(src_input_embeddings, (1-latent_mask[:, :-1])) # flip the mask to instead zero out non-latents
        tgt_input_embeddings = torch.mul(tgt_input_embeddings, (1-latent_mask[:, 1:]))

        src_input_embeddings = torch.add(src_input_embeddings, latent_tensor[:, :-1])
        tgt_input_embeddings = torch.add(tgt_input_embeddings, latent_tensor[:, 1:])

        outputs = model(inputs_embeds=src_input_embeddings, attention_mask=attention_mask[:, 1:], output_hidden_states=True)

        pred_latents = outputs.hidden_states[-1]
        pred_latents = torch.mul(pred_latents, latent_mask[:, 1:, :])

        pred_latents = pred_latents.view(-1, pred_latents.shape[-1])
        tgt_input_embeddings = tgt_input_embeddings.view(-1, tgt_input_embeddings.shape[-1])

        loss = F.mse_loss(pred_latents, tgt_input_embeddings)

        return loss

checkpoint_dir = "./models/latent-cot-sft/llama-3.2-1B-multiplication-sft-final"

optim = torch.optim.Adam(model.parameters(), lr=1e-5)

latent_cot_model = LatentCOT()
model.to(device)
latent_cot_model.to(device)

epochs = 3

import os

progress_bar = tqdm(range(len(cot_latent_dataloader)))

for epoch in range(epochs):
    progress_bar.reset()
    for batch_idx, batch in enumerate(cot_latent_dataloader):
        loss = latent_cot_model(batch['input_ids'], batch['attention_mask'], batch['latent_tensor'], batch['latent_mask'])

        progress_bar.set_description(f"Batch: {batch_idx}, Loss: {loss.item()}")
        optim.zero_grad()
        loss.backward()
        optim.step()

        progress_bar.update(1)

    model.save_pretrained(os.path.join(checkpoint_dir, f"epoch_{epoch}"))
